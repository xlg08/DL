"""
案例:
    Softmax激活函数演示.

回顾: ANN介绍
    概述:
        人工神经网络(Artificial neural network), 通过多个 神经元 搭建的神经网络, 模仿生物学的神经元, 模拟世间万事万物.

    组成:
        输入层, 隐藏层, 输出层,   每层都可以有N个神经元.
        其中 输入层神经元的个数 = 特征的数量.

    细节:
        1. 同层的多个神经元之间相互不连接.
        2. 本层的每个神经元都会和 上一层的所有神经元建立连接, 叫: 全连接层(Linear, FC)
        3. 搭建神经网络时, 只需要搭建 隐藏层 和 输出层即可.
           且: 输入维度是上一层的神经元的数量, 输出维度是本层的神经元的数量.
        4. 关于神经元个数的经验之谈, 浅层的神经元数量可以多一点, 深层的神经元数量可以少一些.
        5. 激活函数的作用是: 给神经元添加 非线性因素, 则神经元就可以处理 分类问题了.

    激活函数分类:
        Sigmoid激活函数:
            既考虑正样本, 也考虑负样本, 切回把结果映射到: [0, 1]之间.
            数据在[-6, 6]之间有效果, 在[-3, 3]之间效果显著, 求导后范围在 [0, 0.25]
            不适用于深层网络, 容易造成: 梯度消失.
            一般应用于 输出层, 且输出层是二分类的.

        Tanh激活函数:
            既考虑正样本, 也考虑负样本, 切把结果映射到: [-1, 1]之间.
            数据在[-3, 3]之间有效果, 在[-1, 1]之间效果显著, 求导后范围在 [0, 1]
            不太适合于深层网络, 容易造成: 梯度消失.
            一般应用于 浅层隐藏层.

        ReLu激活函数:
            (默认)只考虑正样本, 公式为: f(x) = max(0, x), 所以可能会导致 神经元死亡(可以缓解过拟合情况)
            导数范围: <= 0 导数为0,  >0 导数为 1
            应用最多, 且大多应用于 隐藏层.  有变形版 -> Leaky Relu, PRelu...

        Softmax激活函数:
            把多分类的结果用概率来表示, 这这些概率值的累加和为1, 最终选取 概率值最大的那个结果.
"""

# 导包
import torch

# 1.准备数据值, 模拟: 加权求和结果.
# scores = torch.tensor([0.2, 0.02, 0.15, 0.15, 1.3, 0.5, 0.06, 1.1, 0.05, 3.75])

scores = torch.tensor([[0.3, 0.24, 5, -3.1], [-0.1, 0.56, 2.4, 0.35]] )

# 2.通过softmax激活函数来实现多分类. dim = 0, 按行计算
probabilities = torch.softmax(scores, dim=0)
# 3. 输出结果.
print(probabilities)

